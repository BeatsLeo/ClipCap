{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers.optimization import get_scheduler\n",
    "from transformers import BertTokenizer, GPT2LMHeadModel, GPT2Model, CLIPFeatureExtractor, CLIPVisionModel, logging, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.set_verbosity_error()   # 消除未使用权重的warning\n",
    "\n",
    "img = torch.from_numpy(cv2.imread('./images/baby.jpg').transpose(2,0,1))\n",
    "feature_extractor = CLIPFeatureExtractor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "vision_model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
    "text_model = GPT2Model.from_pretrained(\"uer/gpt2-chinese-cluecorpussmall\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 冻结参数\n",
    "# 不训练, 不需要计算梯度\n",
    "for param in vision_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in text_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"uer/gpt2-chinese-cluecorpussmall\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义下游任务模型\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, gpt: GPT2Model, prefix_len, const_len):\n",
    "        super().__init__()\n",
    "        self.prefix_len = prefix_len\n",
    "        self.const_len = const_len\n",
    "        # self.const_embeddings = torch.nn.Parameter(torch.randn(1, const_len, 768))\n",
    "\n",
    "        # self.vision = vit\n",
    "        self.mapping_vision = torch.nn.Linear(257*1024, prefix_len*768)\n",
    "        # self.mapping_prefix = torch.nn.Linear(768, 768)\n",
    "        self.text_gen = gpt\n",
    "        # 加载生成模型生成部分最后一层fc参数\n",
    "        self.fc = torch.nn.Linear(768, tokenizer.vocab_size, bias=False)\n",
    "        parameters = GPT2LMHeadModel.from_pretrained('uer/gpt2-chinese-cluecorpussmall')\n",
    "        self.fc.load_state_dict(parameters.lm_head.state_dict())\n",
    "\n",
    "        self.criterion =torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, img_feature, labels):\n",
    "        bs = img_feature.shape[0]\n",
    "        # 图像特征提取\n",
    "        # with torch.no_grad():\n",
    "            # img_feature = self.vision(pixel_values).last_hidden_state.view(bs, -1)\n",
    "\n",
    "        # 将图像特征map到文本特征\n",
    "        prefix_embeddings = self.mapping_vision(img_feature).view(bs, self.prefix_len, 768)\n",
    "        # prefix_embeddings = torch.concat([prefix_embeddings, self.const_embeddings.expand(bs, self.const_len, 768)], dim=1)\n",
    "        # prefix_embeddings = self.mapping_prefix(prefix_embeddings)\n",
    "        label_embeddings = self.text_gen.wte(labels)\n",
    "        \n",
    "        # 文本生成\n",
    "        logits = torch.concat([prefix_embeddings, label_embeddings], dim=1)\n",
    "        logits = self.text_gen(inputs_embeds = logits).last_hidden_state    # attention_mask默认全1\n",
    "        logits = self.fc(logits)[:, self.prefix_len+self.const_len-1:-1]\n",
    "        \n",
    "        # 计算损失\n",
    "        shift_logits = logits.flatten(end_dim=1)\n",
    "        shift_labels = labels.flatten()\n",
    "\n",
    "        loss = self.criterion(shift_logits, shift_labels)\n",
    "\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'logits': logits\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(text_model, 5, 0).to(device)\n",
    "pixel_values = feature_extractor(img, return_tensors='pt')['pixel_values'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_feature = vision_model(pixel_values).last_hidden_state.view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = tokenizer.encode('我爱的宝宝', return_tensors='pt', add_special_tokens=False).to(device)\n",
    "model(img_feature, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoches = 2\n",
    "model.train()\n",
    "model.text_gen.eval()\n",
    "optimizer = AdamW(model.parameters(), lr = 5e-6)\n",
    "scheduler = get_scheduler(name='linear',\n",
    "                            num_warmup_steps=0,\n",
    "                            num_training_steps=epoches,\n",
    "                            optimizer=optimizer)\n",
    "\n",
    "for i in range(epoches):\n",
    "    loss = model(img_feature, label)['loss']\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # 解决梯度爆炸\n",
    "    \n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    model.zero_grad()\n",
    "\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    global model\n",
    "    device = 'cpu' if torch.cuda.is_available() else 'cpu'\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr = 2e-5)\n",
    "    scheduler = get_scheduler(name='linear',\n",
    "                              num_warmup_steps=0,\n",
    "                              num_training_steps=len(loader),\n",
    "                              optimizer=optimizer)\n",
    "    \n",
    "    model.train()\n",
    "    for i, data in enumerate(loader):\n",
    "        for k in data.keys():\n",
    "            data[k] = data[k].to(device)\n",
    "\n",
    "        out = model(**data)\n",
    "        loss = out['loss']\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # 解决梯度爆炸\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "\n",
    "        if(i % 50 == 0):\n",
    "            labels = data['labels'][:, 1:]\n",
    "            out = out['logits'].argmax(dim=2)[:, :-1]\n",
    "\n",
    "            accuracy = (out == labels).sum().item() / labels.numel()\n",
    "\n",
    "            lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "\n",
    "            print(i, loss.item(), lr, accuracy)\n",
    "\n",
    "    model = model.to('cpu')\n",
    "\n",
    "train()\n",
    "torch.save(model, './models/en_gen.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(img_feature, model, tokenizer, max_length, num_samples):\n",
    "\n",
    "    def generate_loop(logits, output):\n",
    "        with torch.no_grad():\n",
    "            out = model.text_gen(inputs_embeds = logits).last_hidden_state    # attention_mask默认全1\n",
    "            out = model.fc(out)[:, -1]\n",
    "\n",
    "        # 在前num_samples个采样\n",
    "        topk_value = torch.topk(out, num_samples).values\n",
    "        topk_value = topk_value[:, -1].unsqueeze(dim=1)\n",
    "\n",
    "        # 赋值\n",
    "        out = out.masked_fill(out < topk_value, -float('inf'))\n",
    "\n",
    "        # 根据概率采样, 无放回\n",
    "        out = out.softmax(dim=1)\n",
    "        out = out.multinomial(num_samples=1)\n",
    "        if(output is None):\n",
    "            output = out\n",
    "        else:\n",
    "            output = torch.cat([output, out], dim=1)\n",
    "        \n",
    "        #将输出编码\n",
    "        out_embeddings = model.text_gen.wte(out)\n",
    "        logits = torch.cat([logits, out_embeddings], dim=1)\n",
    "\n",
    "        if(logits.shape[1] >= max_length):\n",
    "            return output\n",
    "        \n",
    "        return generate_loop(logits, output)\n",
    "\n",
    "    logits = model.mapping_vision(img_feature).view(1, model.prefix_len, 768)\n",
    "\n",
    "    # 重复5遍\n",
    "    output = None\n",
    "    logits = logits.expand(5, model.prefix_len, 768)\n",
    "    output = generate_loop(logits, output)\n",
    "\n",
    "    for i in range(5):\n",
    "        print(i, tokenizer.decode(output[i].flatten(), add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(img_feature, model, tokenizer, max_length=10, num_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_img(img_path: str, feature_extractor: CLIPFeatureExtractor, vision_model: CLIPVisionModel):\n",
    "    # 导入图片\n",
    "    img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "    print('你输入的图片: ')\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    img = torch.from_numpy(img.transpose(2,0,1))\n",
    "    \n",
    "    # 图片预处理\n",
    "    pixel_values = feature_extractor(img, return_tensors='pt')['pixel_values']\n",
    "    img_feature = vision_model(pixel_values).last_hidden_state.view(1, -1)\n",
    "\n",
    "    # 生成文字\n",
    "    print('生成的文字: ')\n",
    "    generate(img_feature, model, tokenizer, max_length=10, num_samples=5)\n",
    "\n",
    "cap_img('./images/baby.jpg', feature_extractor, vision_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('beatsleo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "16ea64f9ee948d927ad35fd9dd41586a042d593dc7bf73dbea6b47fb27e81f20"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
